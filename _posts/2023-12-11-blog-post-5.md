---
title: 'A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks: An Explainer'
date: 2023-12-11
permalink: /posts/2023/12/ood-baseline-explainer/
tags:
  - OOD Detection
  - Machine Learning
  - Deep Learning
---

The OOD baseline paper titled _"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks"_ is one of the most important research paper in OOD detection literature. Though the paper is not mathematically and technically heavy, it certainly has some concepts that might not be straightforward for beginners. The discussion and/or tutorial of the paper has been [lacking](https://www.reddit.com/r/MachineLearning/comments/13wcopl/d_has_anyone_read_an_old_paper_called_baseline/) in the internet. Hence, to help the beginners dive into the field of out-of-distribution detection and make it easier for beginners to fully understand the concepts of the paper, I have written an explainer blog. 

## What is OOD and failure detection?
OOD and failure detection are separate concepts. The training data has its data distribution depending on its data generation process. The data generation process of the training data might be different than the data generation process of the test data. This difference in data generation process is likely to cause errors in prediction. The process of detecting if a data point belongs to the training data distribution is called out-of-distribution (OOD) detection..

Failure detection, on the other hand, is detecting if a model is likely to make a wrong prediction. Several things other than out-of-distribution can cause failure. Failure can be a result of inherent noise in the data or boundary point or simply inability of a machine learning model. The notion of model flagging some predictions as error-prone can help and increase usability of the model. 

##  Why is OOD and failure detection important?
If we are using a machine learning model to give a user product recommendation such as in Amazon or friend recommendation such as in Facebook, it might not hurt if there are occassional bad recommendations. However, if we are using a machine learning system to detect a disease, the errors might be catastrophic. The cost of a person's life can not be quantified in any measurable terms. So, in critical applications such as medicine, law, and fully autonomous driving, the safety and accuracy of the systems is extremely important. It is way more helpful for the machine learning model to say "I don't know" rather than give wrong predictions in such cases. As the machine learning is starting to be ubiquitous in a lot of fields, the safety and accuracy of the machine learning-based systems is of utmost importance. So, the problem of OOD and failure detection has seen its importance increasingly growing.

##  What is the paper trying to convey?
This is the first question we need to answer. The paper is trying to convey a few important things about predictions of a softmax function.
1. The prediction probability outputted by the softmax function to a bunch of classes can not be literally interpreted as a probability. If a cat/dog classifier predicts 90% probability for cat, it does not mean 9 out of 10 times the ground truth would be cat. Instead, it gives a proxy value of confidence the model has in its prediction.
2. However, the softmax probability can be useful to determine two things:
    - How likely the model's prediction is wrong?
    - How likely the input data is out-of-distribution?
    
    The lower value of softmax probability gives us the signal that the model might not be doing great in the example. This might be because the example is quite difficult for the model or the example might be out-of-distribution.
3. So, they see the statistics of the softmax probability of correct examples (the examples on which the model made right predictions) and wrong examples (the examples on which the model made wrong predictions). They find out that the distribution of the softmax probability is different for correct and wrong examples. They even show the statistical significance of the difference using Wilcoxon rank-sum test. So, this means we can pick some threshold which separates the distribution of correct examples and wrong examples, and then use that to reject to make predictions on _probable_ wrong examples.

